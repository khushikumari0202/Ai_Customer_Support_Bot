{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU+WvoQELFLWY0NrXu9ENV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khushikumari0202/Ai_Customer_Support_Bot/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4LHCOsFXI5_p"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset. The split='train' retrieves the main part of the data.\n",
        "# This may take a moment to download.\n",
        "dataset = load_dataset(\"MakTek/Customer_support_faqs_dataset\", split=\"train\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QFSau1LpJGdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Print the number of examples\n",
        "print(f\"Number of rows: {len(dataset)}\")\n",
        "\n",
        "# 2. Print the column names (features)\n",
        "print(f\"Features: {dataset.column_names}\")\n",
        "\n",
        "# 3. Display the first few rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(dataset[:5])\n",
        "\n",
        "# OPTIONAL: Convert to Pandas DataFrame for easier viewing/manipulation\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "print(\"\\nPandas DataFrame Head:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zvcw_O_AJSoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning and maintaining consistency"
      ],
      "metadata": {
        "id": "qSuXNnMbKaeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 1: COMPLETE, CORRECTED SETUP, DEFINITIONS, AND NON-BLOCKING SERVER STARTUP ---\n",
        "\n",
        "# Install necessary libraries (in case the runtime was restarted)\n",
        "!pip install -q datasets pandas transformers sentence-transformers faiss-cpu google-genai fastapi uvicorn nest_asyncio pyngrok\n",
        "\n",
        "# --- Imports (Consolidated and Corrected) ---\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.responses import RedirectResponse\n",
        "from pydantic import BaseModel\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# --- ADDED NECESSARY IMPORTS FOR THREADING FIX ---\n",
        "import threading\n",
        "import time\n",
        "# --------------------------------------------------\n",
        "\n",
        "# --- 1. RAG COMPONENT INITIALIZATION ---\n",
        "dataset = load_dataset(\"MakTek/Customer_support_faqs_dataset\", split=\"train\")\n",
        "df = dataset.to_pandas()\n",
        "knowledge_base_df = df\n",
        "print(\"Checkpoint 1: Dataset loaded and converted to DataFrame.\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_question'] = df['question'].apply(clean_text)\n",
        "\n",
        "try:\n",
        "    # Initialize SentenceTransformer and Faiss Index\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(df['cleaned_question'].tolist(), convert_to_tensor=False)\n",
        "    embeddings = np.array(embeddings).astype('float32')\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(embeddings)\n",
        "    print(\"Checkpoint 2: RAG components (model, index) initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing RAG components: {e}\")\n",
        "\n",
        "def retrieve_faq(query_text, k=1):\n",
        "    cleaned_query = clean_text(query_text)\n",
        "    query_embedding = model.encode([cleaned_query], convert_to_tensor=False)\n",
        "    query_embedding = np.array(query_embedding).astype('float32')\n",
        "    D, I = index.search(query_embedding, k)\n",
        "\n",
        "    results = []\n",
        "    for row_index in I[0]:\n",
        "        if row_index < 0: continue\n",
        "        faq_q = knowledge_base_df.iloc[row_index]['question']\n",
        "        faq_a = knowledge_base_df.iloc[row_index]['answer']\n",
        "        results.append({\n",
        "            'source_question': faq_q,\n",
        "            'source_answer': faq_a,\n",
        "            'match_score': D[0][I[0].tolist().index(row_index)]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "# --- 2. LLM CLIENT INITIALIZATION ---\n",
        "try:\n",
        "    # Note: Assumes 'GEMINI_API_KEY' is the secret for your Gemini key\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if not GEMINI_API_KEY:\n",
        "        raise ValueError(\"API Key not found in Colab Secrets.\")\n",
        "\n",
        "    # DEBUG: Key status check\n",
        "    print(f\"DEBUG: Key status: Read successfully. Length: {len(GEMINI_API_KEY)} characters.\")\n",
        "\n",
        "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "    LLM_NAME = 'gemini-2.5-flash'\n",
        "    print(f\"Checkpoint 3: Gemini client initialized using {LLM_NAME}.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"FATAL ERROR: Could not initialize Gemini client: {e}\")\n",
        "    client = None\n",
        "\n",
        "# --- MODIFIED SYSTEM_PROMPT FOR CONTEXTUAL MEMORY ---\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an AI Customer Support Bot. Your task is to provide concise, direct, and helpful answers to customer questions.\n",
        "You MUST ONLY use the provided 'CONTEXT' (Retrieved FAQs) and the 'HISTORY' (Previous conversation) to formulate your answer.\n",
        "The HISTORY is provided to understand the CONTEXT of the CURRENT user question.\n",
        "\n",
        "If the provided CONTEXT does not contain the answer, you must state:\n",
        "\"I apologize, I was unable to find a definitive answer in our FAQs. I will now simulate an escalation to a human agent.\"\n",
        "Do not use any external knowledge.\n",
        "\"\"\"\n",
        "# ----------------------------------------------------\n",
        "\n",
        "\n",
        "# --- MODIFIED generate_rag_response FUNCTION (ADDED conversation_context) ---\n",
        "def generate_rag_response(user_query, k=1, conversation_context=\"\"):\n",
        "    if client is None:\n",
        "        return \"LLM service not initialized. Cannot generate response.\"\n",
        "\n",
        "    retrieved_faqs = retrieve_faq(user_query, k=k)\n",
        "\n",
        "    if not retrieved_faqs:\n",
        "        context = \"No relevant FAQs found.\"\n",
        "    else:\n",
        "        context = \"\\n---\\n\".join([f\"Question: {item['source_question']}\\nAnswer: {item['source_answer']}\" for item in retrieved_faqs])\n",
        "\n",
        "    # Check for escalation based on low relevance score (> 5 is poor match)\n",
        "    if \"No relevant FAQs found\" in context or (retrieved_faqs and retrieved_faqs[0]['match_score'] > 5):\n",
        "        final_answer = \"I apologize, I was unable to find a definitive answer in our FAQs. I will now simulate an escalation to a human agent.\"\n",
        "    else:\n",
        "        # --- MODIFIED PROMPT CONSTRUCTION TO INCLUDE HISTORY ---\n",
        "        prompt = f\"\"\"\n",
        "        HISTORY:\n",
        "        {conversation_context}\n",
        "\n",
        "        CONTEXT (Retrieved FAQs):\n",
        "        {context}\n",
        "\n",
        "        CURRENT USER QUESTION: {user_query}\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=LLM_NAME, contents=[SYSTEM_PROMPT, prompt], config={\"temperature\": 0.0}\n",
        "            )\n",
        "            final_answer = response.text\n",
        "        except Exception as e:\n",
        "            final_answer = f\"An error occurred during LLM generation: {e}\"\n",
        "    return final_answer\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# --- 3. FASTAPI DEFINITION ---\n",
        "app = FastAPI(title=\"AI Customer Support Bot API\")\n",
        "session_history = {}\n",
        "MAX_CONTEXT_LENGTH = 5\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    session_id: str\n",
        "    user_message: str\n",
        "\n",
        "def get_conversation_context(session_id):\n",
        "    history = session_history.get(session_id, [])\n",
        "    recent_history = history[-MAX_CONTEXT_LENGTH:]\n",
        "    context_str = \"Recent Conversation History:\\n\"\n",
        "    for message in recent_history:\n",
        "        context_str += f\"- {message['role'].capitalize()}: {message['text']}\\n\"\n",
        "    return context_str.strip()\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "def chat_endpoint(request: ChatRequest):\n",
        "    session_id = request.session_id\n",
        "    user_message = request.user_message\n",
        "\n",
        "    if session_id not in session_history:\n",
        "        session_history[session_id] = []\n",
        "\n",
        "    # --- MODIFIED CHAT ENDPOINT FOR CONTEXTUAL MEMORY ---\n",
        "    # 1. Calculate context string BEFORE adding the current user message\n",
        "    context_str = get_conversation_context(session_id)\n",
        "\n",
        "    # 2. Add user message to history\n",
        "    session_history[session_id].append({\"role\": \"user\", \"text\": user_message})\n",
        "\n",
        "    # 3. Pass the context to the RAG function\n",
        "    final_response = generate_rag_response(user_message, k=2, conversation_context=context_str)\n",
        "    # ----------------------------------------------------\n",
        "\n",
        "    if \"simulate an escalation\" in final_response:\n",
        "        bot_response = final_response\n",
        "    else:\n",
        "        bot_response = final_response\n",
        "\n",
        "    session_history[session_id].append({\"role\": \"bot\", \"text\": bot_response})\n",
        "    return {\"session_id\": session_id, \"response\": bot_response}\n",
        "\n",
        "@app.get(\"/history/{session_id}\")\n",
        "def get_history(session_id: str):\n",
        "    if session_id not in session_history:\n",
        "        raise HTTPException(status_code=404, detail=\"Session ID not found\")\n",
        "    return {\"session_id\": session_id, \"history\": session_history[session_id]}\n",
        "\n",
        "# Corrected: New endpoint to redirect root URL to /docs\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    \"\"\"Redirects to the OpenAPI (Swagger) documentation page.\"\"\"\n",
        "    return RedirectResponse(url=\"/docs\")\n",
        "\n",
        "\n",
        "# --- DEFINITION OF NON-BLOCKING SERVER FUNCTION ---\n",
        "def start_uvicorn():\n",
        "    \"\"\"Function to run uvicorn in a separate thread.\"\"\"\n",
        "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"error\")\n",
        "    server = uvicorn.Server(config)\n",
        "    # The server.run() call blocks, so it must be in a thread\n",
        "    server.run()\n",
        "# ----------------------------------------------------\n",
        "\n",
        "\n",
        "# --- 4. SERVER STARTUP (THIS LINE MUST EXECUTE LAST) ---\n",
        "# Apply patch for running FastAPI in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Stop any previous ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Get ngrok authtoken from Colab secrets\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    print(\"NGROK_AUTH_TOKEN not found in Colab Secrets. Please add it to run the server.\")\n",
        "else:\n",
        "    print(\"Checkpoint 4: NGROK_AUTH_TOKEN found. Attempting to start tunnel.\")\n",
        "\n",
        "    # Configure ngrok to bypass local configuration issues\n",
        "    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "    temp_config = conf.PyngrokConfig(\n",
        "        auth_token=conf.get_default().auth_token,\n",
        "        ngrok_path=conf.get_default().ngrok_path,\n",
        "        config_path=None\n",
        "    )\n",
        "    conf.set_default(temp_config)\n",
        "\n",
        "    try:\n",
        "        # 1. Start Ngrok Tunnel\n",
        "        NGROK_TUNNEL = ngrok.connect(8000, proto='http')\n",
        "        public_url = NGROK_TUNNEL.public_url\n",
        "\n",
        "        print(\"Checkpoint 5: Ngrok tunnel established.\")\n",
        "        print(f\"\\n--- FastAPI Server is running ---\")\n",
        "        print(f\"Access the public URL at: {public_url}\")\n",
        "        print(\"-\" * 35)\n",
        "\n",
        "        # 2. Start Uvicorn in a background thread\n",
        "        print(\"Checkpoint 6: Starting Uvicorn server in a separate thread.\")\n",
        "        server_thread = threading.Thread(target=start_uvicorn)\n",
        "        server_thread.start()\n",
        "\n",
        "        # Give the server a moment to start before returning control\n",
        "        time.sleep(10)\n",
        "        print(\"Checkpoint 7: Uvicorn thread started. Main kernel is now free to run Cell 2.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nServer startup failed: {e}\")"
      ],
      "metadata": {
        "id": "naN047eCJYNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL 2: CONTEXTUAL MEMORY TEST ---\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "NGROK_BASE_URL = \"https://unmonitored-perorational-nathaniel.ngrok-free.dev\"\n",
        "\n",
        "BASE_URL = f\"{NGROK_BASE_URL}/chat\"\n",
        "context_session_id = f\"context_test_{int(time.time())}\"\n",
        "\n",
        "print(f\"Starting Context Test (Session: {context_session_id})\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Step 1: Ask an initial question (e.g., about payment options)\n",
        "query_a = \"what are the ways to pay for my order\"\n",
        "payload_a = {\"session_id\": context_session_id, \"user_message\": query_a}\n",
        "response_a = requests.post(BASE_URL, json=payload_a).json()\n",
        "print(f\"User 1: {query_a}\")\n",
        "print(f\"Bot 1: {response_a.get('response')}\\n\")\n",
        "\n",
        "# Step 2: Ask a follow-up question that relies on context (e.g., using a pronoun like 'it' or 'that')\n",
        "query_b = \"Can I use that for my refund?\"\n",
        "payload_b = {\"session_id\": context_session_id, \"user_message\": query_b}\n",
        "response_b = requests.post(BASE_URL, json=payload_b).json()\n",
        "\n",
        "# EXPECTED: The bot must answer based on the retrieved FAQs combined with the history.\n",
        "# If it answers with an escalation message, the contextual memory failed.\n",
        "print(f\"User 2: {query_b}\")\n",
        "print(f\"Bot 2: {response_b.get('response')}\\n\")\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"--- History Check ---\")\n",
        "history_response = requests.get(f\"{NGROK_BASE_URL}/history/{context_session_id}\").json()\n",
        "for item in history_response.get('history', []):\n",
        "    print(f\"[{item['role'].capitalize()}]: {item['text'][:70].replace('\\n', ' ')}...\")"
      ],
      "metadata": {
        "id": "QDfOpjJfxNAV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}