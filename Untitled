{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMU+WvoQELFLWY0NrXu9ENV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"4LHCOsFXI5_p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760538552200,"user_tz":-330,"elapsed":11934,"user":{"displayName":"khushi kumari","userId":"18210048452732667740"}},"outputId":"298a5db0-7a1a-491d-dc57-089b49c6742c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["!pip install datasets\n"]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load the dataset. The split='train' retrieves the main part of the data.\n","# This may take a moment to download.\n","dataset = load_dataset(\"MakTek/Customer_support_faqs_dataset\", split=\"train\")"],"metadata":{"collapsed":true,"id":"QFSau1LpJGdM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760538557984,"user_tz":-330,"elapsed":5782,"user":{"displayName":"khushi kumari","userId":"18210048452732667740"}},"outputId":"15097917-c39e-43f0-f80a-913a3b10bf4c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# 1. Print the number of examples\n","print(f\"Number of rows: {len(dataset)}\")\n","\n","# 2. Print the column names (features)\n","print(f\"Features: {dataset.column_names}\")\n","\n","# 3. Display the first few rows\n","print(\"\\nFirst 5 rows:\")\n","print(dataset[:5])\n","\n","# OPTIONAL: Convert to Pandas DataFrame for easier viewing/manipulation\n","import pandas as pd\n","df = pd.DataFrame(dataset)\n","\n","print(\"\\nPandas DataFrame Head:\")\n","print(df.head())"],"metadata":{"collapsed":true,"id":"zvcw_O_AJSoK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760538558004,"user_tz":-330,"elapsed":17,"user":{"displayName":"khushi kumari","userId":"18210048452732667740"}},"outputId":"bd89aafc-e586-4fe7-b9b9-0536beff8448"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of rows: 200\n","Features: ['question', 'answer']\n","\n","First 5 rows:\n","{'question': ['How can I create an account?', 'What payment methods do you accept?', 'How can I track my order?', 'What is your return policy?', 'Can I cancel my order?'], 'answer': [\"To create an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process.\", 'We accept major credit cards, debit cards, and PayPal as payment methods for online orders.', \"You can track your order by logging into your account and navigating to the 'Order History' section. There, you will find the tracking information for your shipment.\", 'Our return policy allows you to return products within 30 days of purchase for a full refund, provided they are in their original condition and packaging. Please refer to our Returns page for detailed instructions.', 'You can cancel your order if it has not been shipped yet. Please contact our customer support team with your order details, and we will assist you with the cancellation process.']}\n","\n","Pandas DataFrame Head:\n","                              question  \\\n","0         How can I create an account?   \n","1  What payment methods do you accept?   \n","2            How can I track my order?   \n","3          What is your return policy?   \n","4               Can I cancel my order?   \n","\n","                                              answer  \n","0  To create an account, click on the 'Sign Up' b...  \n","1  We accept major credit cards, debit cards, and...  \n","2  You can track your order by logging into your ...  \n","3  Our return policy allows you to return product...  \n","4  You can cancel your order if it has not been s...  \n"]}]},{"cell_type":"markdown","source":["Cleaning and maintaining consistency"],"metadata":{"id":"qSuXNnMbKaeS"}},{"cell_type":"code","source":["# --- CELL 1: COMPLETE, CORRECTED SETUP, DEFINITIONS, AND NON-BLOCKING SERVER STARTUP ---\n","\n","# Install necessary libraries (in case the runtime was restarted)\n","!pip install -q datasets pandas transformers sentence-transformers faiss-cpu google-genai fastapi uvicorn nest_asyncio pyngrok\n","\n","# --- Imports (Consolidated and Corrected) ---\n","from fastapi import FastAPI, HTTPException\n","from fastapi.responses import RedirectResponse\n","from pydantic import BaseModel\n","import pandas as pd\n","from datasets import load_dataset\n","import re\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","import numpy as np\n","import os\n","from google import genai\n","from google.colab import userdata\n","import uvicorn\n","import nest_asyncio\n","import asyncio\n","from pyngrok import ngrok, conf\n","\n","# --- ADDED NECESSARY IMPORTS FOR THREADING FIX ---\n","import threading\n","import time\n","# --------------------------------------------------\n","\n","# --- 1. RAG COMPONENT INITIALIZATION ---\n","dataset = load_dataset(\"MakTek/Customer_support_faqs_dataset\", split=\"train\")\n","df = dataset.to_pandas()\n","knowledge_base_df = df\n","print(\"Checkpoint 1: Dataset loaded and converted to DataFrame.\")\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","df['cleaned_question'] = df['question'].apply(clean_text)\n","\n","try:\n","    # Initialize SentenceTransformer and Faiss Index\n","    model = SentenceTransformer('all-MiniLM-L6-v2')\n","    embeddings = model.encode(df['cleaned_question'].tolist(), convert_to_tensor=False)\n","    embeddings = np.array(embeddings).astype('float32')\n","    d = embeddings.shape[1]\n","    index = faiss.IndexFlatL2(d)\n","    index.add(embeddings)\n","    print(\"Checkpoint 2: RAG components (model, index) initialized.\")\n","except Exception as e:\n","    print(f\"Error initializing RAG components: {e}\")\n","\n","def retrieve_faq(query_text, k=1):\n","    cleaned_query = clean_text(query_text)\n","    query_embedding = model.encode([cleaned_query], convert_to_tensor=False)\n","    query_embedding = np.array(query_embedding).astype('float32')\n","    D, I = index.search(query_embedding, k)\n","\n","    results = []\n","    for row_index in I[0]:\n","        if row_index < 0: continue\n","        faq_q = knowledge_base_df.iloc[row_index]['question']\n","        faq_a = knowledge_base_df.iloc[row_index]['answer']\n","        results.append({\n","            'source_question': faq_q,\n","            'source_answer': faq_a,\n","            'match_score': D[0][I[0].tolist().index(row_index)]\n","        })\n","    return results\n","\n","\n","# --- 2. LLM CLIENT INITIALIZATION ---\n","try:\n","    # Note: Assumes 'GEMINI_API_KEY' is the secret for your Gemini key\n","    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n","    if not GEMINI_API_KEY:\n","        raise ValueError(\"API Key not found in Colab Secrets.\")\n","\n","    # DEBUG: Key status check\n","    print(f\"DEBUG: Key status: Read successfully. Length: {len(GEMINI_API_KEY)} characters.\")\n","\n","    client = genai.Client(api_key=GEMINI_API_KEY)\n","    LLM_NAME = 'gemini-2.5-flash'\n","    print(f\"Checkpoint 3: Gemini client initialized using {LLM_NAME}.\")\n","\n","except Exception as e:\n","    print(f\"FATAL ERROR: Could not initialize Gemini client: {e}\")\n","    client = None\n","\n","# --- MODIFIED SYSTEM_PROMPT FOR CONTEXTUAL MEMORY ---\n","SYSTEM_PROMPT = \"\"\"\n","You are an AI Customer Support Bot. Your task is to provide concise, direct, and helpful answers to customer questions.\n","You MUST ONLY use the provided 'CONTEXT' (Retrieved FAQs) and the 'HISTORY' (Previous conversation) to formulate your answer.\n","The HISTORY is provided to understand the CONTEXT of the CURRENT user question.\n","\n","If the provided CONTEXT does not contain the answer, you must state:\n","\"I apologize, I was unable to find a definitive answer in our FAQs. I will now simulate an escalation to a human agent.\"\n","Do not use any external knowledge.\n","\"\"\"\n","# ----------------------------------------------------\n","\n","\n","# --- MODIFIED generate_rag_response FUNCTION (ADDED conversation_context) ---\n","def generate_rag_response(user_query, k=1, conversation_context=\"\"):\n","    if client is None:\n","        return \"LLM service not initialized. Cannot generate response.\"\n","\n","    retrieved_faqs = retrieve_faq(user_query, k=k)\n","\n","    if not retrieved_faqs:\n","        context = \"No relevant FAQs found.\"\n","    else:\n","        context = \"\\n---\\n\".join([f\"Question: {item['source_question']}\\nAnswer: {item['source_answer']}\" for item in retrieved_faqs])\n","\n","    # Check for escalation based on low relevance score (> 5 is poor match)\n","    if \"No relevant FAQs found\" in context or (retrieved_faqs and retrieved_faqs[0]['match_score'] > 5):\n","        final_answer = \"I apologize, I was unable to find a definitive answer in our FAQs. I will now simulate an escalation to a human agent.\"\n","    else:\n","        # --- MODIFIED PROMPT CONSTRUCTION TO INCLUDE HISTORY ---\n","        prompt = f\"\"\"\n","        HISTORY:\n","        {conversation_context}\n","\n","        CONTEXT (Retrieved FAQs):\n","        {context}\n","\n","        CURRENT USER QUESTION: {user_query}\n","        \"\"\"\n","        try:\n","            response = client.models.generate_content(\n","                model=LLM_NAME, contents=[SYSTEM_PROMPT, prompt], config={\"temperature\": 0.0}\n","            )\n","            final_answer = response.text\n","        except Exception as e:\n","            final_answer = f\"An error occurred during LLM generation: {e}\"\n","    return final_answer\n","# --------------------------------------------------------------------------\n","\n","\n","# --- 3. FASTAPI DEFINITION ---\n","app = FastAPI(title=\"AI Customer Support Bot API\")\n","session_history = {}\n","MAX_CONTEXT_LENGTH = 5\n","\n","class ChatRequest(BaseModel):\n","    session_id: str\n","    user_message: str\n","\n","def get_conversation_context(session_id):\n","    history = session_history.get(session_id, [])\n","    recent_history = history[-MAX_CONTEXT_LENGTH:]\n","    context_str = \"Recent Conversation History:\\n\"\n","    for message in recent_history:\n","        context_str += f\"- {message['role'].capitalize()}: {message['text']}\\n\"\n","    return context_str.strip()\n","\n","@app.post(\"/chat\")\n","def chat_endpoint(request: ChatRequest):\n","    session_id = request.session_id\n","    user_message = request.user_message\n","\n","    if session_id not in session_history:\n","        session_history[session_id] = []\n","\n","    # --- MODIFIED CHAT ENDPOINT FOR CONTEXTUAL MEMORY ---\n","    # 1. Calculate context string BEFORE adding the current user message\n","    context_str = get_conversation_context(session_id)\n","\n","    # 2. Add user message to history\n","    session_history[session_id].append({\"role\": \"user\", \"text\": user_message})\n","\n","    # 3. Pass the context to the RAG function\n","    final_response = generate_rag_response(user_message, k=2, conversation_context=context_str)\n","    # ----------------------------------------------------\n","\n","    if \"simulate an escalation\" in final_response:\n","        bot_response = final_response\n","    else:\n","        bot_response = final_response\n","\n","    session_history[session_id].append({\"role\": \"bot\", \"text\": bot_response})\n","    return {\"session_id\": session_id, \"response\": bot_response}\n","\n","@app.get(\"/history/{session_id}\")\n","def get_history(session_id: str):\n","    if session_id not in session_history:\n","        raise HTTPException(status_code=404, detail=\"Session ID not found\")\n","    return {\"session_id\": session_id, \"history\": session_history[session_id]}\n","\n","# Corrected: New endpoint to redirect root URL to /docs\n","@app.get(\"/\")\n","def read_root():\n","    \"\"\"Redirects to the OpenAPI (Swagger) documentation page.\"\"\"\n","    return RedirectResponse(url=\"/docs\")\n","\n","\n","# --- DEFINITION OF NON-BLOCKING SERVER FUNCTION ---\n","def start_uvicorn():\n","    \"\"\"Function to run uvicorn in a separate thread.\"\"\"\n","    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"error\")\n","    server = uvicorn.Server(config)\n","    # The server.run() call blocks, so it must be in a thread\n","    server.run()\n","# ----------------------------------------------------\n","\n","\n","# --- 4. SERVER STARTUP (THIS LINE MUST EXECUTE LAST) ---\n","# Apply patch for running FastAPI in Colab\n","nest_asyncio.apply()\n","\n","# Stop any previous ngrok tunnels\n","ngrok.kill()\n","\n","# Get ngrok authtoken from Colab secrets\n","NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n","\n","if not NGROK_AUTH_TOKEN:\n","    print(\"NGROK_AUTH_TOKEN not found in Colab Secrets. Please add it to run the server.\")\n","else:\n","    print(\"Checkpoint 4: NGROK_AUTH_TOKEN found. Attempting to start tunnel.\")\n","\n","    # Configure ngrok to bypass local configuration issues\n","    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n","    temp_config = conf.PyngrokConfig(\n","        auth_token=conf.get_default().auth_token,\n","        ngrok_path=conf.get_default().ngrok_path,\n","        config_path=None\n","    )\n","    conf.set_default(temp_config)\n","\n","    try:\n","        # 1. Start Ngrok Tunnel\n","        NGROK_TUNNEL = ngrok.connect(8000, proto='http')\n","        public_url = NGROK_TUNNEL.public_url\n","\n","        print(\"Checkpoint 5: Ngrok tunnel established.\")\n","        print(f\"\\n--- FastAPI Server is running ---\")\n","        print(f\"Access the public URL at: {public_url}\")\n","        print(\"-\" * 35)\n","\n","        # 2. Start Uvicorn in a background thread\n","        print(\"Checkpoint 6: Starting Uvicorn server in a separate thread.\")\n","        server_thread = threading.Thread(target=start_uvicorn)\n","        server_thread.start()\n","\n","        # Give the server a moment to start before returning control\n","        time.sleep(10)\n","        print(\"Checkpoint 7: Uvicorn thread started. Main kernel is now free to run Cell 2.\")\n","\n","    except Exception as e:\n","        print(f\"\\nServer startup failed: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naN047eCJYNX","outputId":"1f7d2911-6476-4d99-8845-11ec57214794","executionInfo":{"status":"ok","timestamp":1760538615896,"user_tz":-330,"elapsed":57879,"user":{"displayName":"khushi kumari","userId":"18210048452732667740"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Checkpoint 1: Dataset loaded and converted to DataFrame.\n","Checkpoint 2: RAG components (model, index) initialized.\n","DEBUG: Key status: Read successfully. Length: 39 characters.\n","Checkpoint 3: Gemini client initialized using gemini-2.5-flash.\n","Checkpoint 4: NGROK_AUTH_TOKEN found. Attempting to start tunnel.\n","Checkpoint 5: Ngrok tunnel established.\n","\n","--- FastAPI Server is running ---\n","Access the public URL at: https://unmonitored-perorational-nathaniel.ngrok-free.dev\n","-----------------------------------\n","Checkpoint 6: Starting Uvicorn server in a separate thread.\n","Checkpoint 7: Uvicorn thread started. Main kernel is now free to run Cell 2.\n"]}]},{"cell_type":"code","source":["# --- CELL 2: CONTEXTUAL MEMORY TEST ---\n","\n","import requests\n","import json\n","import time\n","\n","NGROK_BASE_URL = \"https://unmonitored-perorational-nathaniel.ngrok-free.dev\"\n","\n","BASE_URL = f\"{NGROK_BASE_URL}/chat\"\n","context_session_id = f\"context_test_{int(time.time())}\"\n","\n","print(f\"Starting Context Test (Session: {context_session_id})\")\n","print(\"=\"*50)\n","\n","# Step 1: Ask an initial question (e.g., about payment options)\n","query_a = \"what are the ways to pay for my order\"\n","payload_a = {\"session_id\": context_session_id, \"user_message\": query_a}\n","response_a = requests.post(BASE_URL, json=payload_a).json()\n","print(f\"User 1: {query_a}\")\n","print(f\"Bot 1: {response_a.get('response')}\\n\")\n","\n","# Step 2: Ask a follow-up question that relies on context (e.g., using a pronoun like 'it' or 'that')\n","query_b = \"Can I use that for my refund?\"\n","payload_b = {\"session_id\": context_session_id, \"user_message\": query_b}\n","response_b = requests.post(BASE_URL, json=payload_b).json()\n","\n","# EXPECTED: The bot must answer based on the retrieved FAQs combined with the history.\n","# If it answers with an escalation message, the contextual memory failed.\n","print(f\"User 2: {query_b}\")\n","print(f\"Bot 2: {response_b.get('response')}\\n\")\n","\n","print(\"=\"*50)\n","print(\"--- History Check ---\")\n","history_response = requests.get(f\"{NGROK_BASE_URL}/history/{context_session_id}\").json()\n","for item in history_response.get('history', []):\n","    print(f\"[{item['role'].capitalize()}]: {item['text'][:70].replace('\\n', ' ')}...\")"],"metadata":{"id":"QDfOpjJfxNAV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760538619593,"user_tz":-330,"elapsed":3683,"user":{"displayName":"khushi kumari","userId":"18210048452732667740"}},"outputId":"dd85a02e-bae9-4a93-9e43-11124e7b140d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Context Test (Session: context_test_1760538619)\n","==================================================\n","User 1: what are the ways to pay for my order\n","Bot 1: We accept major credit cards, debit cards, and PayPal as payment methods for online orders.\n","\n","User 2: Can I use that for my refund?\n","Bot 2: I apologize, I was unable to find a definitive answer in our FAQs. I will now simulate an escalation to a human agent.\n","\n","==================================================\n","--- History Check ---\n","[User]: what are the ways to pay for my order...\n","[Bot]: We accept major credit cards, debit cards, and PayPal as payment metho...\n","[User]: Can I use that for my refund?...\n","[Bot]: I apologize, I was unable to find a definitive answer in our FAQs. I w...\n"]}]}]}